---
title: "Introduction to Deep Learning"
topic: true
subjects: ['Python']
subjects_weight: 1
draft: true
intro: |
  Deep Learning is a vast and convoluted topic. It’s hard to know where to start. This workshop will help you take your first steps with Deep Learning.

  The workshop will introduce you to the fundamental concepts behind Deep Learning and show you how to get started building models using Python and Keras. You'll learn some of the underlying maths (a PhD in Mathematics will not be required!) and work through several examples.

  You’ll walk away with an appreciation for what’s possible with Deep Learning and sufficient hands-on experience to start building your own models.

  All material will be available as Jupyter Notebooks.
duration: 1 day
who: This workshop is aimed at people with little or no prior experience with Deep Learning. If you're already a Deep Learning ninja, then this is not for you!
# objectives: 
# outcomes:
requirements: Familiarity with programming in Python. A basic understanding of Machine Learning concepts will be helpful but certainly not essential.
setup: |
  You'll need the following to get the most out of the workshop:

  - a laptop and
  - an account on [Google Colab](https://colab.research.google.com/).

  If you have not used Jupyter Notebooks before, then quickly read through the following resources:

  - [What is the Jupyter Notebook?](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/What%20is%20the%20Jupyter%20Notebook.ipynb)
  - [Notebook Basics](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb)
  - [Running Code](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Running%20Code.ipynb) and
  - [Markdown Cells](https://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Working%20With%20Markdown%20Cells.ipynb).

  Watch the following videos for some context:

  - [Gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)
  - [Gradient and graphs](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient-and-graphs)
---

<!--
	https://medium.com/intro-to-artificial-intelligence/deep-learning-series-1-intro-to-deep-learning-abb1780ee20
	https://medium.com/intro-to-artificial-intelligence/simple-image-classification-using-deep-learning-deep-learning-series-2-5e5b89e97926
	https://medium.com/intro-to-artificial-intelligence/traffic-sign-detection-selefdriving-car-deep-learning-series-3-1db4eda67979
	https://realpython.com/python-keras-text-classification/
	https://towardsdatascience.com/an-introduction-to-deep-learning-af63448c122c
	https://skymind.ai/wiki/lstm
	https://adventuresinmachinelearning.com/keras-lstm-tutorial/
	https://skymind.ai/wiki/generative-adversarial-network-gan
	https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900
	https://towardsdatascience.com/image-generator-drawing-cartoons-with-generative-adversarial-networks-45e814ca9b6b
-->

- Introduction to Neural Networks
	- Weights and bias
	- Activation functions
	- Loss functions
	- Chain rule and back-propagation
	- Project — Simple binary classifier
	- Where neural networks fail: images
- Deep Learning Libraries
	- TensorFlow
	- Keras
- Convolutional Neural Networks
	- Convolution layers
	- Filters and padding
	- Pooling
	- Activation functions: sigmoid, relu and softmax
	- Project — Text classification
	- Project — Image classification
- Recurrent Neural Networks
	- Back-propagation through time
	- Long Short-Term Memory (LSTM)
	- Project — Text prediction
