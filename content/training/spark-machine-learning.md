---
title: "Machine Learning at Scale with PySpark"
topic: true
subjects: ['Spark']
draft: false
---

Apache Spark is a fast, general-purpose system for cluster computing. Learn how to use Python and Spark to easily build Machine Learning models on large datasets.

In this workshop you’ll learn how to ingest structured data into Spark, then create classification and regression models. You’ll also discover how to use pipelines to streamline your workflow and how these can be combined with cross-validation and grid-search to easily optimise model parameters.

All material will be available as Jupyter Notebooks.

## Contents

- Machine Learning and Big Data
- Connecting to Spark
- Loading data
	- `DataFrame`
	- Spark SQL
- Working with data
	- Categorical data
		- Indexing
		- One-hot encoding
		- Dense versus Sparse
	- Data preparation
		- Column manipulation
		- Bucketing
		- Assembling columns
	- Text data
		- Punctuation and numbers
		- Tokens
		- Stop words
		- Hashing
		- TF-IDF
- Classification	
	- Decision Tree
	- Logistic Regression
- Regression
	- Linear regression
	- Penalised regression
- Pipelines
- Cross-validation
- Grid search
- Ensemble models

## Prerequisites

In order for this course to make sense you should first complete the [Introduction to Spark]({{< ref "spark-introduction.md" >}}) course.